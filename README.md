# 📘 Introduction to Linear Algebra for AI

Welcome to the **Introduction to Linear Algebra for AI** course! This course is designed to provide a comprehensive understanding of linear algebra concepts crucial for artificial intelligence and machine learning.

## 📜 Syllabus

### Part A: Basics of Linear Algebra
- **What is Vector, Matrix, Tensor?**
- **Vector Space**:
  - Space, vector space
  - Linear combination
  - Span
  - Linear dependence and independence
  - Basis
  - Dimension
- **Column and Row Vectors**:
  - Span in matrix form
  - Column space
  - Row space
  - Null vector and null space
  - Left null space
  - Operations on matrices (matrix multiplication, vectorization, and Hadamard product)
  - Inner and outer product
- **Rank**:
  - Column rank, row rank, rank, full rank
  - Fat and thin matrix
  - Rank deficient, singular, nonsingular, inverse
- **Orthogonality**:
  - Length of vector
  - Orthogonal vector, orthogonal basis
  - Orthonormal
  - Orthogonal matrix
  - Gram-Schmidt orthogonalization
- **Functions in Linear Algebra**:
  - Affine function
  - Quadratic function

### Part B: Norms & Normalizations
- **Vector Norms**:
  - LP-norms
- **Matrix Norms**:
  - Frobenius norm
  - Spectral norm
  - Nuclear norm
- **Importance of Normalization in Machine Learning**:
  - Batching data
  - Batch norm
  - Layer norm
  - Standardization
  - Comparison

### Part C: Equations
- **Systems of Equations**:
  - Under-determined system
  - Over-determined system
- **Linear Equations**:
  - Full rank case
  - Low rank (rank deficient) case
  - Noisy case
  - Least squares method

### Part D: Eigens and Positive Definity
- **Eigenvalues and Eigenvectors**:
  - Algebraic multiplicity
  - Geometric multiplicity
  - Eigenspace
  - Eigen decomposition
- **Positive Definite and Positive Semi-Definite**:
  - Properties of PD and PSD matrices

### Part E: SVD & Special Matrices
- **Singular Value Decomposition (SVD)**:
  - SVD
  - Skinny SVD
  - Compact SVD
- **Statistics and Matrices**:
  - Normal distribution
  - Variance
  - Covariance matrix
  - Correlation matrix
  - Modal matrix
  - Making features of datasets independent
  - Properties of eigenvalues and eigenvectors of covariance matrix
  - Scale invariant
  - Translation invariant
  - Rotation invariant
 

### Part F: Additional Topics
- **Data Storage**:
  - Storing data row major
- **Vectorization**:
  - Vectorization in numpy

> **Note**: This course does not cover other study topics such as functions, transformations, eliminations, decompositions, projections, and multivariate calculus (Gradient, Hessian, Jacobian).

## 🌟 Key Features
- 📚 Comprehensive content tailored for AI enthusiasts
- 🔍 Detailed explanations with practical examples
- 🧮 Mathematical foundations essential for machine learning

## 📫 Contact
Feel free to reach out if you have any questions or suggestions:
- **Email**: mehrant.0611@gmail.com
- **GitHub**: [shining0611armor](https://github.com/shining0611armor)

---

Happy Learning! 😊
